{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0f4e23",
   "metadata": {
    "_cell_guid": "a72542a8-c07a-4115-b3bd-fbf92a052535",
    "_uuid": "7b342edb-e379-49ff-a10f-5f2bf837dd9d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-01-05T21:55:01.313870Z",
     "iopub.status.busy": "2022-01-05T21:55:01.312397Z",
     "iopub.status.idle": "2022-01-05T21:55:02.837699Z",
     "shell.execute_reply": "2022-01-05T21:55:02.838324Z",
     "shell.execute_reply.started": "2022-01-05T21:54:08.085711Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.537034,
     "end_time": "2022-01-05T21:55:02.838902",
     "exception": false,
     "start_time": "2022-01-05T21:55:01.301868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def getConfig():\n",
    "    return {'model_name': '../input/checkpoint20000/checkpoint-20000',   \n",
    "         'token_name': '../input/py-bigbird-v26',\n",
    "         'max_length': 1024,\n",
    "         'train_batch_size':12,\n",
    "         'valid_batch_size':8,\n",
    "         'epochs':20,\n",
    "         'learning_rates': [1e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if torch.cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2325af54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T21:55:02.879230Z",
     "iopub.status.busy": "2022-01-05T21:55:02.878231Z",
     "iopub.status.idle": "2022-01-05T21:55:08.470098Z",
     "shell.execute_reply": "2022-01-05T21:55:08.470507Z",
     "shell.execute_reply.started": "2022-01-05T21:54:08.094369Z"
    },
    "papermill": {
     "duration": 5.612169,
     "end_time": "2022-01-05T21:55:08.470697",
     "exception": false,
     "start_time": "2022-01-05T21:55:02.858528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
    "\n",
    "# Return an array that maps character index to index of word in list of split() words\n",
    "\n",
    "\n",
    "def split_mapping(unsplit):\n",
    "    splt = unsplit.split()\n",
    "    offset_to_wordidx = np.full(len(unsplit), -1)\n",
    "    txt_ptr = 0\n",
    "    for split_index, full_word in enumerate(splt):\n",
    "        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "            txt_ptr += 1\n",
    "        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "        txt_ptr += len(full_word)\n",
    "    return offset_to_wordidx\n",
    "\n",
    "\n",
    "def loadFromCSV(path=f'./train_NER.csv'):\n",
    "    train_text_df = pd.read_csv(path)\n",
    "    # pandas saves lists as string, we must convert back\n",
    "    train_text_df.entities = train_text_df.entities.apply(lambda x: eval(x))\n",
    "    return train_text_df\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, get_wids,standard=False):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids  # for validation\n",
    "        output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim',\n",
    "                         'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "        self.labels_to_ids = {v: k for k, v in enumerate(output_labels)}\n",
    "        self.ids_to_labels = {k: v for k, v in enumerate(output_labels)}\n",
    "        print(self.ids_to_labels)\n",
    "        self.standard=standard\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS\n",
    "        text = self.data.text[index]\n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        word_ids = encoding.word_ids()\n",
    "        split_word_ids = np.full(len(word_ids), -1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "\n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "\n",
    "            if word_idx is None:\n",
    "                if not self.get_wids:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0, 0):\n",
    "                    # Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx]\n",
    "                                                   [0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(\n",
    "                        split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "\n",
    "                    if split_index != -1:\n",
    "                        if not self.get_wids:\n",
    "                            label_ids.append(\n",
    "                                self.labels_to_ids[word_labels[split_index]])\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and self.ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids:\n",
    "                                label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if not self.get_wids:\n",
    "                                label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids:\n",
    "                        label_ids.append(-100)\n",
    "\n",
    "        encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids:\n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "\n",
    "        if(self.standard):\n",
    "            del item[\"offset_mapping\"]\n",
    "            return item\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def getSets(standard=False):\n",
    "    train_df = pd.read_csv('corrected.csv')\n",
    "    # CHOOSE VALIDATION INDEXES (that match my TF notebook)\n",
    "    IDS = train_df.id.unique()\n",
    "    print('There are', len(IDS),\n",
    "          'train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "    # TRAIN VALID SPLIT 90% 10%\n",
    "    np.random.seed(42)\n",
    "    train_idx = np.random.choice(\n",
    "        np.arange(len(IDS)), int(0.9*len(IDS)), replace=False)\n",
    "    valid_idx = np.setdiff1d(np.arange(len(IDS)), train_idx)\n",
    "    np.random.seed(None)\n",
    "\n",
    "    # CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "    train_text_df = loadFromCSV()\n",
    "    config = getConfig()\n",
    "    print(train_text_df.head())\n",
    "    data = train_text_df[['id', 'text', 'entities']]\n",
    "    train_dataset = data.loc[data['id'].isin(\n",
    "        IDS[train_idx]), ['text', 'entities']].reset_index(drop=True)\n",
    "    test_dataset = data.loc[data['id'].isin(\n",
    "        IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(data.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "    training_set = dataset(train_dataset, tokenizer,\n",
    "                           config['max_length'], False,standard)\n",
    "    testing_set = dataset(test_dataset, tokenizer, config['max_length'], True,standard)\n",
    "\n",
    "    if standard:\n",
    "        return training_set, testing_set\n",
    "    else:\n",
    "        # TRAIN DATASET AND VALID DATASET\n",
    "        train_params = {'batch_size': config['train_batch_size'],\n",
    "                        'shuffle': True,\n",
    "                        'num_workers': 2,\n",
    "                        'pin_memory': True\n",
    "                        }\n",
    "\n",
    "        test_params = {'batch_size': config['valid_batch_size'],\n",
    "                       'shuffle': False,\n",
    "                       'num_workers': 2,\n",
    "                       'pin_memory': True\n",
    "                       }\n",
    "\n",
    "        training_loader = DataLoader(training_set, **train_params)\n",
    "        testing_loader = DataLoader(testing_set, **test_params)\n",
    "        return train_dataset, training_loader, test_dataset, testing_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82934f35",
   "metadata": {
    "papermill": {
     "duration": 0.00548,
     "end_time": "2022-01-05T21:55:08.482394",
     "exception": false,
     "start_time": "2022-01-05T21:55:08.476914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f447c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T21:55:08.516540Z",
     "iopub.status.busy": "2022-01-05T21:55:08.506345Z",
     "iopub.status.idle": "2022-01-05T21:55:09.200915Z",
     "shell.execute_reply": "2022-01-05T21:55:09.200385Z",
     "shell.execute_reply.started": "2022-01-05T21:54:08.192023Z"
    },
    "papermill": {
     "duration": 0.713112,
     "end_time": "2022-01-05T21:55:09.201047",
     "exception": false,
     "start_time": "2022-01-05T21:55:08.487935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BigBirdForTokenClassification, BigBirdConfig,TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}\n",
    "\n",
    "def inference(model, config, batch):\n",
    "    global ids_to_labels\n",
    "    # MOVE BATCH TO GPU AND INFER\n",
    "    ids = batch[\"input_ids\"].to(config['device'])\n",
    "    mask = batch[\"attention_mask\"].to(config['device'])\n",
    "    outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy()\n",
    "\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    predictions = []\n",
    "    for k, text_preds in enumerate(all_preds):\n",
    "        token_preds = [ids_to_labels[i] for i in text_preds]\n",
    "\n",
    "        prediction = []\n",
    "        word_ids = batch['wids'][k].numpy()\n",
    "        previous_word_idx = -1\n",
    "        for idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:\n",
    "                prediction.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_predictions(model, config, df, loader):\n",
    "    # put model in training mode\n",
    "    model.eval()\n",
    "\n",
    "    # GET WORD LABEL PREDICTIONS\n",
    "    y_pred2 = []\n",
    "    for batch in loader:\n",
    "        labels = inference(model, config, batch)\n",
    "        y_pred2.extend(labels)\n",
    "\n",
    "    final_preds2 = []\n",
    "    for i in range(len(df)):\n",
    "        idx = df.id.values[i]\n",
    "        # prpoch\n",
    "        d = [x.replace('B-', '').replace('I-', '') for x in y_pred2[i]]\n",
    "        pred = y_pred2[i]  # Leave \"B\" and \"I\"\n",
    "        preds = []\n",
    "        j = 0\n",
    "        while j < len(pred):\n",
    "            cls = pred[j]\n",
    "            # The commented out line below appears to be a bug.\n",
    "#             if cls == 'O': j += 1\n",
    "            if cls == 'O':\n",
    "                pass\n",
    "            else:\n",
    "                cls = cls.replace('B', 'I')  # spans start with B\n",
    "            end = j + 1\n",
    "            while end < len(pred) and pred[end] == cls:\n",
    "                end += 1\n",
    "\n",
    "            if cls != 'O' and cls != '' and end - j > 7:\n",
    "                final_preds2.append((idx, cls.replace('I-', ''),\n",
    "                                     ' '.join(map(str, list(range(j, end))))))\n",
    "\n",
    "            j = end\n",
    "\n",
    "    oof = pd.DataFrame(final_preds2)\n",
    "    oof.columns = ['id', 'class', 'new_predictionstring']\n",
    "\n",
    "    return oof\n",
    "\n",
    "\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.new_predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.new_predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter / len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "\n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id', 'discourse_type', 'new_predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id', 'class', 'new_predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id', 'class'],\n",
    "                           right_on=['id', 'discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred', '_gt')\n",
    "                           )\n",
    "    joined['new_predictionstring_gt'] = joined['new_predictionstring_gt'].fillna(' ')\n",
    "    joined['new_predictionstring_pred'] = joined['new_predictionstring_pred'].fillna(\n",
    "        ' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (\n",
    "        joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1', 'overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id', 'new_predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique()\n",
    "                   if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique()\n",
    "                        if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    # calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score\n",
    "\n",
    "\n",
    "def inferencePipeline(test_dataset,testing_loader,model,config):  # note this doesn't run during submit\n",
    "    \n",
    "    train_df = pd.read_csv('corrected.csv')\n",
    "    IDS = train_df.id.unique()\n",
    "    np.random.seed(42)\n",
    "    train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "    valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "    np.random.seed(None)\n",
    "\n",
    "    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
    "\n",
    "    # OOF PREDICTIONS\n",
    "    oof = get_predictions(model,config,test_dataset, testing_loader)\n",
    "\n",
    "    # COMPUTE F1 SCORE\n",
    "    f1s = []\n",
    "    CLASSES = oof['class'].unique()\n",
    "    print()\n",
    "    for c in CLASSES:\n",
    "        pred_df = oof.loc[oof['class'] == c].copy()\n",
    "        gt_df = valid.loc[valid['discourse_type'] == c].copy()\n",
    "        f1 = score_feedback_comp(pred_df, gt_df)\n",
    "        print(c, f1)\n",
    "        f1s.append(f1)\n",
    "    print()\n",
    "    print('Overall', np.mean(f1s))\n",
    "    print()\n",
    "    return np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d12417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T21:55:09.217803Z",
     "iopub.status.busy": "2022-01-05T21:55:09.214687Z",
     "iopub.status.idle": "2022-01-05T21:55:09.220836Z",
     "shell.execute_reply": "2022-01-05T21:55:09.220389Z",
     "shell.execute_reply.started": "2022-01-05T21:54:08.227257Z"
    },
    "papermill": {
     "duration": 0.013988,
     "end_time": "2022-01-05T21:55:09.220943",
     "exception": false,
     "start_time": "2022-01-05T21:55:09.206955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "def load_files(path):\n",
    "    # Load the text files from the dir and build a Dataframe.\n",
    "    names,text=[],[]\n",
    "    for f in tqdm(list(os.listdir(path))):\n",
    "        names.append(f.replace('.txt',''))\n",
    "        text.append(open(path+f,'r').read())\n",
    "    texts=pd.DataFrame({\"id\":names,\"text\":text})\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e32d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T21:55:09.280566Z",
     "iopub.status.busy": "2022-01-05T21:55:09.235431Z",
     "iopub.status.idle": "2022-01-05T21:55:23.601867Z",
     "shell.execute_reply": "2022-01-05T21:55:23.601344Z",
     "shell.execute_reply.started": "2022-01-05T21:54:08.239916Z"
    },
    "papermill": {
     "duration": 14.375533,
     "end_time": "2022-01-05T21:55:23.602000",
     "exception": false,
     "start_time": "2022-01-05T21:55:09.226467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 175.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Lead', 2: 'I-Lead', 3: 'B-Position', 4: 'I-Position', 5: 'B-Claim', 6: 'I-Claim', 7: 'B-Counterclaim', 8: 'I-Counterclaim', 9: 'B-Rebuttal', 10: 'I-Rebuttal', 11: 'B-Evidence', 12: 'I-Evidence', 13: 'B-Concluding Statement', 14: 'I-Concluding Statement'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "config = getConfig()\n",
    "model = AutoModelForTokenClassification.from_pretrained(config['model_name'],num_labels=15)\n",
    "model.to(config[\"device\"])\n",
    "testDF=load_files(\"../input/feedback-prize-2021/test/\")\n",
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "                   'shuffle': False,\n",
    "                   'num_workers': 2,\n",
    "                   'pin_memory': True\n",
    "                   }\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['token_name'])\n",
    "test_texts_set = dataset(testDF, tokenizer, config['max_length'], True)\n",
    "test_texts_loader = DataLoader(test_texts_set, **test_params)\n",
    "sub = get_predictions(model, config, testDF, test_texts_loader)\n",
    "sub.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.303948,
   "end_time": "2022-01-05T21:55:27.238927",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-05T21:54:52.934979",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
